{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tiles3 as tc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(q_values):\n",
    "    top = float(\"-inf\")\n",
    "    ties = []\n",
    "\n",
    "    for i in range(len(q_values)):\n",
    "        if q_values[i] > top:\n",
    "            top = q_values[i]\n",
    "            ties = []\n",
    "\n",
    "        if q_values[i] == top:\n",
    "            ties.append(i)\n",
    "\n",
    "    return np.random.choice(ties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleTileCoder:\n",
    "    def __init__(self, iht_size=4096, num_tilings=8, num_tiles=8):\n",
    "        \"\"\"\n",
    "        Initializes the MountainCar Tile Coder\n",
    "        Initializers:\n",
    "        iht_size -- int, the size of the index hash table, typically a power of 2\n",
    "        num_tilings -- int, the number of tilings\n",
    "        num_tiles -- int, the number of tiles. Here both the width and height of the\n",
    "                     tile coder are the same\n",
    "        Class Variables:\n",
    "        self.iht -- tc.IHT, the index hash table that the tile coder will use\n",
    "        self.num_tilings -- int, the number of tilings the tile coder will use\n",
    "        self.num_tiles -- int, the number of tiles the tile coder will use\n",
    "        \"\"\"\n",
    "        self.iht = tc.IHT(iht_size)\n",
    "        self.num_tilings = num_tilings\n",
    "        self.num_tiles = num_tiles\n",
    "    \n",
    "    def get_tiles(self, position, velocity, angle, angular_vel):\n",
    "        \"\"\"\n",
    "        Takes in a position and velocity from the mountaincar environment\n",
    "        and returns a numpy array of active tiles.\n",
    "        \n",
    "        Arguments:\n",
    "        position -- float, the position of the agent between -1.2 and 0.5\n",
    "        velocity -- float, the velocity of the agent between -0.07 and 0.07\n",
    "        returns:\n",
    "        tiles - np.array, active tiles\n",
    "        \"\"\"\n",
    "\n",
    "        POSITION_MIN = -4.8\n",
    "        POSITION_MAX = 4.8\n",
    "        VELOCITY_MIN = -2\n",
    "        VELOCITY_MAX = 2\n",
    "        POLE_ANGLE_MIN = -0.418\n",
    "        POLE_ANGLE_MAX = 0.418\n",
    "        POLE_ANG_VEL_MIN = -2\n",
    "        POLE_ANG_VEL_MAX = 2\n",
    "\n",
    "        position_scale = self.num_tiles / (POSITION_MAX - POSITION_MIN)\n",
    "        velocity_scale = self.num_tiles / (VELOCITY_MAX - VELOCITY_MIN)\n",
    "        pole_angle_scale = self.num_tiles / (POLE_ANGLE_MAX - POLE_ANGLE_MIN)\n",
    "        pole_angle_velocity_scale = self.num_tiles / (POLE_ANG_VEL_MAX - POLE_ANG_VEL_MIN)\n",
    "\n",
    "        tiles = tc.tiles(self.iht, self.num_tilings, [position * position_scale, \n",
    "                                                      velocity * velocity_scale,\n",
    "                                                      angle * pole_angle_scale,\n",
    "                                                      angular_vel * pole_angle_velocity_scale])\n",
    "        \n",
    "        return np.array(tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "    \n",
    "    num_tilings = 8\n",
    "    num_tiles = 8\n",
    "    iht_size = 4096 # index hash table\n",
    "    \n",
    "    # learning rate\n",
    "    alpha = 0.5 / num_tilings\n",
    "    # how often random move\n",
    "    epsilon = 0.1\n",
    "    # discount future rewards\n",
    "    gamma = 1\n",
    "    \n",
    "    \n",
    "\n",
    "    def __init__(self, action_space, observation_space):\n",
    "\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        # initialize value-function weights w\n",
    "        self.w = np.ones((action_space.n, self.iht_size))\n",
    "        \n",
    "        # initialize tilecoder\n",
    "        self.tc = CartPoleTileCoder(iht_size=self.iht_size, \n",
    "                                       num_tilings=self.num_tilings, \n",
    "                                       num_tiles=self.num_tiles)\n",
    "        \n",
    "        self.previous_tiles = None\n",
    "        \n",
    "        # initialize state, action and reward\n",
    "        #self.s = self.a = self.r = None\n",
    "    \n",
    "    def select_initial_action(self, state):\n",
    "        position, velocity, angle, angular_vel = state\n",
    "        \n",
    "        active_tiles = self.tc.get_tiles(position, velocity, angle, angular_vel)\n",
    "        action, action_value = self.select_action(active_tiles)\n",
    "        \n",
    "        self.previous_tiles = np.copy(active_tiles)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def select_action(self, tiles):\n",
    "        action_values = []\n",
    "        action = None\n",
    "        # First loop through the weights of each action and populate action_values\n",
    "        # with the action value for each action and tiles instance\n",
    "        for w in self.w:\n",
    "            action_values.append(sum(w[tiles]))\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(len(action_values))\n",
    "        else:\n",
    "            action = argmax(action_values)\n",
    "        \n",
    "        #print(\"tiles\", tiles, \"action_values\", action_values)\n",
    "        #print(\"weights\", self.w)\n",
    "        \n",
    "        return action, action_values[action]\n",
    "\n",
    "    def observe(self, state, action, reward, next_state, done):\n",
    "            #input()\n",
    "            #print(\"state\", state, \"action\", action, \"reward\", reward, \"next_state\", next_state)\n",
    "            if done:\n",
    "                target = reward\n",
    "                estimate = sum(self.w[action][self.previous_tiles])\n",
    "                self.w[action][self.previous_tiles] += self.alpha * (target - estimate)\n",
    "                return None\n",
    "            else:\n",
    "                position, velocity, angle, angular_vel = next_state\n",
    "                active_tiles = self.tc.get_tiles(position, velocity, angle, angular_vel)\n",
    "                \n",
    "                next_action, action_value = self.select_action(active_tiles)\n",
    "                \n",
    "                target = reward + self.gamma * action_value\n",
    "                estimate = sum(self.w[action][self.previous_tiles])\n",
    "                # print(\"loss\", target - estimate)\n",
    "                \n",
    "                self.w[action][self.previous_tiles] += self.alpha * (target - estimate)\n",
    "                \n",
    "                self.previous_tiles = np.copy(active_tiles)\n",
    "            return next_action\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 28 timesteps with reward 28.0\n",
      "Episode finished after 46 timesteps with reward 46.0\n",
      "Episode finished after 168 timesteps with reward 168.0\n",
      "Episode finished after 91 timesteps with reward 91.0\n",
      "Episode finished after 88 timesteps with reward 88.0\n",
      "Episode finished after 159 timesteps with reward 159.0\n",
      "Episode finished after 77 timesteps with reward 77.0\n",
      "Episode finished after 109 timesteps with reward 109.0\n",
      "Episode finished after 120 timesteps with reward 120.0\n",
      "Episode finished after 42 timesteps with reward 42.0\n",
      "Episode finished after 158 timesteps with reward 158.0\n",
      "Episode finished after 104 timesteps with reward 104.0\n",
      "Episode finished after 191 timesteps with reward 191.0\n",
      "Episode finished after 182 timesteps with reward 182.0\n",
      "Episode finished after 136 timesteps with reward 136.0\n",
      "Episode finished after 62 timesteps with reward 62.0\n",
      "Episode finished after 160 timesteps with reward 160.0\n",
      "Episode finished after 184 timesteps with reward 184.0\n",
      "Episode finished after 87 timesteps with reward 87.0\n",
      "Episode finished after 186 timesteps with reward 186.0\n",
      "Episode finished after 223 timesteps with reward 223.0\n",
      "Episode finished after 229 timesteps with reward 229.0\n",
      "Episode finished after 269 timesteps with reward 269.0\n",
      "Episode finished after 194 timesteps with reward 194.0\n",
      "Episode finished after 318 timesteps with reward 318.0\n",
      "Episode finished after 240 timesteps with reward 240.0\n",
      "Episode finished after 239 timesteps with reward 239.0\n",
      "Episode finished after 180 timesteps with reward 180.0\n",
      "Episode finished after 223 timesteps with reward 223.0\n",
      "Episode finished after 324 timesteps with reward 324.0\n",
      "Episode finished after 241 timesteps with reward 241.0\n",
      "Episode finished after 253 timesteps with reward 253.0\n",
      "Episode finished after 232 timesteps with reward 232.0\n",
      "Episode finished after 225 timesteps with reward 225.0\n",
      "Episode finished after 208 timesteps with reward 208.0\n",
      "Episode finished after 237 timesteps with reward 237.0\n",
      "Episode finished after 240 timesteps with reward 240.0\n",
      "Episode finished after 247 timesteps with reward 247.0\n",
      "Episode finished after 212 timesteps with reward 212.0\n",
      "Episode finished after 231 timesteps with reward 231.0\n",
      "Episode finished after 52 timesteps with reward 52.0\n",
      "Episode finished after 211 timesteps with reward 211.0\n",
      "Episode finished after 224 timesteps with reward 224.0\n",
      "Episode finished after 261 timesteps with reward 261.0\n",
      "Episode finished after 223 timesteps with reward 223.0\n",
      "Episode finished after 285 timesteps with reward 285.0\n",
      "Episode finished after 274 timesteps with reward 274.0\n",
      "Episode finished after 275 timesteps with reward 275.0\n",
      "Episode finished after 228 timesteps with reward 228.0\n",
      "Episode finished after 330 timesteps with reward 330.0\n",
      "Episode finished after 318 timesteps with reward 318.0\n",
      "Episode finished after 305 timesteps with reward 305.0\n",
      "Episode finished after 241 timesteps with reward 241.0\n",
      "Episode finished after 207 timesteps with reward 207.0\n",
      "Episode finished after 329 timesteps with reward 329.0\n",
      "Episode finished after 257 timesteps with reward 257.0\n",
      "Episode finished after 361 timesteps with reward 361.0\n",
      "Episode finished after 178 timesteps with reward 178.0\n",
      "Episode finished after 270 timesteps with reward 270.0\n",
      "Episode finished after 270 timesteps with reward 270.0\n",
      "Episode finished after 225 timesteps with reward 225.0\n",
      "Episode finished after 245 timesteps with reward 245.0\n",
      "Episode finished after 272 timesteps with reward 272.0\n",
      "Episode finished after 308 timesteps with reward 308.0\n",
      "Episode finished after 543 timesteps with reward 543.0\n",
      "Episode finished after 320 timesteps with reward 320.0\n",
      "Episode finished after 269 timesteps with reward 269.0\n",
      "Episode finished after 325 timesteps with reward 325.0\n",
      "Episode finished after 430 timesteps with reward 430.0\n",
      "Episode finished after 317 timesteps with reward 317.0\n",
      "Episode finished after 375 timesteps with reward 375.0\n",
      "Episode finished after 241 timesteps with reward 241.0\n",
      "Episode finished after 235 timesteps with reward 235.0\n",
      "Episode finished after 211 timesteps with reward 211.0\n",
      "Episode finished after 293 timesteps with reward 293.0\n",
      "Episode finished after 348 timesteps with reward 348.0\n",
      "Episode finished after 258 timesteps with reward 258.0\n",
      "Episode finished after 339 timesteps with reward 339.0\n",
      "Episode finished after 1048 timesteps with reward 1048.0\n",
      "Episode finished after 322 timesteps with reward 322.0\n",
      "Episode finished after 309 timesteps with reward 309.0\n",
      "Episode finished after 442 timesteps with reward 442.0\n",
      "Episode finished after 357 timesteps with reward 357.0\n",
      "Episode finished after 513 timesteps with reward 513.0\n",
      "Episode finished after 277 timesteps with reward 277.0\n",
      "Episode finished after 247 timesteps with reward 247.0\n",
      "Episode finished after 330 timesteps with reward 330.0\n",
      "Episode finished after 290 timesteps with reward 290.0\n",
      "Episode finished after 279 timesteps with reward 279.0\n",
      "Episode finished after 312 timesteps with reward 312.0\n",
      "Episode finished after 243 timesteps with reward 243.0\n",
      "Episode finished after 213 timesteps with reward 213.0\n",
      "Episode finished after 258 timesteps with reward 258.0\n",
      "Episode finished after 340 timesteps with reward 340.0\n",
      "Episode finished after 541 timesteps with reward 541.0\n",
      "Episode finished after 741 timesteps with reward 741.0\n",
      "Episode finished after 285 timesteps with reward 285.0\n",
      "Episode finished after 271 timesteps with reward 271.0\n",
      "Episode finished after 256 timesteps with reward 256.0\n",
      "Episode finished after 349 timesteps with reward 349.0\n",
      "Episode finished after 349 timesteps with reward 349.0\n",
      "Episode finished after 604 timesteps with reward 604.0\n",
      "Episode finished after 656 timesteps with reward 656.0\n",
      "Episode finished after 544 timesteps with reward 544.0\n",
      "Episode finished after 406 timesteps with reward 406.0\n",
      "Episode finished after 262 timesteps with reward 262.0\n",
      "Episode finished after 410 timesteps with reward 410.0\n",
      "Episode finished after 1173 timesteps with reward 1173.0\n",
      "Episode finished after 864 timesteps with reward 864.0\n",
      "Episode finished after 861 timesteps with reward 861.0\n",
      "Episode finished after 469 timesteps with reward 469.0\n",
      "Episode finished after 302 timesteps with reward 302.0\n",
      "Episode finished after 532 timesteps with reward 532.0\n",
      "Episode finished after 515 timesteps with reward 515.0\n",
      "Episode finished after 724 timesteps with reward 724.0\n",
      "Episode finished after 579 timesteps with reward 579.0\n",
      "Episode finished after 392 timesteps with reward 392.0\n",
      "Episode finished after 273 timesteps with reward 273.0\n",
      "Episode finished after 633 timesteps with reward 633.0\n",
      "Episode finished after 650 timesteps with reward 650.0\n",
      "Episode finished after 227 timesteps with reward 227.0\n",
      "Episode finished after 410 timesteps with reward 410.0\n",
      "Episode finished after 430 timesteps with reward 430.0\n",
      "Episode finished after 811 timesteps with reward 811.0\n",
      "Episode finished after 2120 timesteps with reward 2120.0\n",
      "Episode finished after 40 timesteps with reward 40.0\n",
      "Episode finished after 521 timesteps with reward 521.0\n",
      "Episode finished after 341 timesteps with reward 341.0\n",
      "Episode finished after 311 timesteps with reward 311.0\n",
      "Episode finished after 678 timesteps with reward 678.0\n",
      "Episode finished after 639 timesteps with reward 639.0\n",
      "Episode finished after 640 timesteps with reward 640.0\n",
      "Episode finished after 544 timesteps with reward 544.0\n",
      "Episode finished after 913 timesteps with reward 913.0\n",
      "Episode finished after 567 timesteps with reward 567.0\n",
      "Episode finished after 1626 timesteps with reward 1626.0\n",
      "Episode finished after 482 timesteps with reward 482.0\n",
      "Episode finished after 759 timesteps with reward 759.0\n",
      "Episode finished after 240 timesteps with reward 240.0\n",
      "Episode finished after 1323 timesteps with reward 1323.0\n",
      "Episode finished after 1456 timesteps with reward 1456.0\n",
      "Episode finished after 534 timesteps with reward 534.0\n",
      "Episode finished after 630 timesteps with reward 630.0\n",
      "Episode finished after 603 timesteps with reward 603.0\n",
      "Episode finished after 90 timesteps with reward 90.0\n",
      "Episode finished after 510 timesteps with reward 510.0\n",
      "Episode finished after 882 timesteps with reward 882.0\n",
      "Episode finished after 432 timesteps with reward 432.0\n",
      "Episode finished after 544 timesteps with reward 544.0\n",
      "Episode finished after 1066 timesteps with reward 1066.0\n",
      "Episode finished after 311 timesteps with reward 311.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 333 timesteps with reward 333.0\n",
      "Episode finished after 296 timesteps with reward 296.0\n",
      "Episode finished after 280 timesteps with reward 280.0\n",
      "Episode finished after 308 timesteps with reward 308.0\n",
      "Episode finished after 176 timesteps with reward 176.0\n",
      "Episode finished after 359 timesteps with reward 359.0\n",
      "Episode finished after 304 timesteps with reward 304.0\n",
      "Episode finished after 796 timesteps with reward 796.0\n",
      "Episode finished after 2397 timesteps with reward 2397.0\n",
      "Episode finished after 678 timesteps with reward 678.0\n",
      "Episode finished after 297 timesteps with reward 297.0\n",
      "Episode finished after 646 timesteps with reward 646.0\n",
      "Episode finished after 506 timesteps with reward 506.0\n",
      "Episode finished after 3131 timesteps with reward 3131.0\n",
      "Episode finished after 280 timesteps with reward 280.0\n",
      "Episode finished after 767 timesteps with reward 767.0\n",
      "Episode finished after 504 timesteps with reward 504.0\n",
      "Episode finished after 740 timesteps with reward 740.0\n",
      "Episode finished after 1316 timesteps with reward 1316.0\n",
      "Episode finished after 1143 timesteps with reward 1143.0\n",
      "Episode finished after 923 timesteps with reward 923.0\n",
      "Episode finished after 540 timesteps with reward 540.0\n",
      "Episode finished after 500 timesteps with reward 500.0\n",
      "Episode finished after 473 timesteps with reward 473.0\n",
      "Episode finished after 691 timesteps with reward 691.0\n",
      "Episode finished after 898 timesteps with reward 898.0\n",
      "Episode finished after 650 timesteps with reward 650.0\n",
      "Episode finished after 529 timesteps with reward 529.0\n",
      "Episode finished after 730 timesteps with reward 730.0\n",
      "Episode finished after 904 timesteps with reward 904.0\n",
      "Episode finished after 1162 timesteps with reward 1162.0\n",
      "Episode finished after 383 timesteps with reward 383.0\n",
      "Episode finished after 434 timesteps with reward 434.0\n",
      "Episode finished after 1135 timesteps with reward 1135.0\n",
      "Episode finished after 63 timesteps with reward 63.0\n",
      "Episode finished after 518 timesteps with reward 518.0\n",
      "Episode finished after 571 timesteps with reward 571.0\n",
      "Episode finished after 3394 timesteps with reward 3394.0\n",
      "Episode finished after 4262 timesteps with reward 4262.0\n",
      "Episode finished after 821 timesteps with reward 821.0\n",
      "Episode finished after 423 timesteps with reward 423.0\n",
      "Episode finished after 1028 timesteps with reward 1028.0\n",
      "Episode finished after 2732 timesteps with reward 2732.0\n",
      "Episode finished after 842 timesteps with reward 842.0\n",
      "Episode finished after 1582 timesteps with reward 1582.0\n",
      "Episode finished after 278 timesteps with reward 278.0\n",
      "Episode finished after 1223 timesteps with reward 1223.0\n",
      "Episode finished after 943 timesteps with reward 943.0\n",
      "Episode finished after 1718 timesteps with reward 1718.0\n",
      "Episode finished after 1744 timesteps with reward 1744.0\n",
      "Episode finished after 463 timesteps with reward 463.0\n",
      "Episode finished after 567 timesteps with reward 567.0\n",
      "Episode finished after 7841 timesteps with reward 7841.0\n",
      "Episode finished after 382 timesteps with reward 382.0\n",
      "Episode finished after 101 timesteps with reward 101.0\n",
      "Episode finished after 663 timesteps with reward 663.0\n",
      "Episode finished after 1765 timesteps with reward 1765.0\n",
      "Episode finished after 1948 timesteps with reward 1948.0\n",
      "Episode finished after 585 timesteps with reward 585.0\n",
      "Episode finished after 2954 timesteps with reward 2954.0\n",
      "Episode finished after 3238 timesteps with reward 3238.0\n",
      "Episode finished after 381 timesteps with reward 381.0\n",
      "Episode finished after 995 timesteps with reward 995.0\n",
      "Episode finished after 4464 timesteps with reward 4464.0\n",
      "Episode finished after 3005 timesteps with reward 3005.0\n",
      "Episode finished after 4178 timesteps with reward 4178.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 2657 timesteps with reward 2657.0\n",
      "Episode finished after 9100 timesteps with reward 9100.0\n",
      "Episode finished after 2751 timesteps with reward 2751.0\n",
      "Episode finished after 638 timesteps with reward 638.0\n",
      "Episode finished after 662 timesteps with reward 662.0\n",
      "Episode finished after 393 timesteps with reward 393.0\n",
      "Episode finished after 8548 timesteps with reward 8548.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 4462 timesteps with reward 4462.0\n",
      "Episode finished after 4013 timesteps with reward 4013.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 4798 timesteps with reward 4798.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 436 timesteps with reward 436.0\n",
      "Episode finished after 804 timesteps with reward 804.0\n",
      "Episode finished after 8968 timesteps with reward 8968.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 4882 timesteps with reward 4882.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 8064 timesteps with reward 8064.0\n",
      "Episode finished after 8606 timesteps with reward 8606.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 8049 timesteps with reward 8049.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 9117 timesteps with reward 9117.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 7503 timesteps with reward 7503.0\n",
      "Episode finished after 3025 timesteps with reward 3025.0\n",
      "Episode finished after 284 timesteps with reward 284.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 4428 timesteps with reward 4428.0\n",
      "Episode finished after 6318 timesteps with reward 6318.0\n",
      "Episode finished after 4428 timesteps with reward 4428.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 4173 timesteps with reward 4173.0\n",
      "Episode finished after 239 timesteps with reward 239.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 6426 timesteps with reward 6426.0\n",
      "Episode finished after 2663 timesteps with reward 2663.0\n",
      "Episode finished after 944 timesteps with reward 944.0\n",
      "Episode finished after 5838 timesteps with reward 5838.0\n",
      "Episode finished after 769 timesteps with reward 769.0\n",
      "Episode finished after 342 timesteps with reward 342.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 4710 timesteps with reward 4710.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 5360 timesteps with reward 5360.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 8166 timesteps with reward 8166.0\n",
      "Episode finished after 734 timesteps with reward 734.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 3562 timesteps with reward 3562.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 7353 timesteps with reward 7353.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 3220 timesteps with reward 3220.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 7277 timesteps with reward 7277.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 1933 timesteps with reward 1933.0\n",
      "Episode finished after 6258 timesteps with reward 6258.0\n",
      "Episode finished after 1656 timesteps with reward 1656.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 2270 timesteps with reward 2270.0\n",
      "Episode finished after 6091 timesteps with reward 6091.0\n",
      "Episode finished after 1477 timesteps with reward 1477.0\n",
      "Episode finished after 8368 timesteps with reward 8368.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 9845 timesteps with reward 9845.0\n",
      "Episode finished after 10000 timesteps with reward 10000.0\n",
      "Episode finished after 533 timesteps with reward 533.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-9a2f647a2b42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# Take action and observe reward, next_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-582e31be7549>\u001b[0m in \u001b[0;36mobserve\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;31m# print(\"loss\", target - estimate)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprevious_tiles\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mestimate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprevious_tiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactive_tiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped\n",
    "agent = SarsaAgent(env.action_space, env.observation_space)\n",
    "\n",
    "total_rewards = []\n",
    "\n",
    "for i_episode in range(1000):\n",
    "    # choose inital state and action\n",
    "    state = env.reset()\n",
    "    action = agent.select_initial_action(state)\n",
    "    total_reward = 0\n",
    "    \n",
    "    for t in range(10000):\n",
    "        # env.render()\n",
    "        # Take action and observe reward, next_state\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        action = agent.observe(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    print(\"Episode finished after {} timesteps with reward {}\".format(t+1, total_reward))\n",
    "    total_rewards.append(total_reward)\n",
    "    #if i_episode >= 100 and np.mean(total_rewards[-100:]) > 195:\n",
    "    #    print(\"Solved, episode\", i_episode)\n",
    "    #    break\n",
    "    \n",
    "env.close()\n",
    "\n",
    "np.mean(total_rewards[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(total_rewards)\n",
    "plt.ylabel('Total reward')\n",
    "plt.xlabel('Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 10000\n",
      "Episode finished after 10000\n",
      "Episode finished after 2679\n",
      "Episode finished after 10000\n",
      "Episode finished after 10000\n"
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "\n",
    "for i_episode in range(10):\n",
    "    # choose inital state and action\n",
    "    state = env.reset()\n",
    "    action = agent.select_initial_action(state)\n",
    "    total_reward = 0\n",
    "    \n",
    "    for t in range(10000):\n",
    "        env.render()\n",
    "        # Take action and observe reward, next_state\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        action = agent.observe(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    \n",
    "    total_rewards.append(total_reward)\n",
    "    print(\"Episode finished after {}\".format(t+1))\n",
    "    \n",
    "    \n",
    "np.mean(total_rewards)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0007741   0.0099058  -0.03934276  0.03623549]\n",
      "[ 0.00097222 -0.18463053 -0.03861805  0.3162504 ]\n",
      "[-0.00272039  0.0110196  -0.03229304  0.01164312]\n",
      "[-0.0025     -0.1836247  -0.03206018  0.29396484]\n",
      "[-0.0061725   0.01193932 -0.02618088 -0.00865452]\n",
      "[-0.00593371  0.20742677 -0.02635397 -0.3094815 ]\n",
      "[-0.00178517  0.4029141  -0.0325436  -0.61035785]\n",
      "[ 0.00627311  0.2082618  -0.04475076 -0.32810012]\n",
      "[ 0.01043834  0.01380456 -0.05131276 -0.04985862]\n",
      "[ 0.01071444  0.2096233  -0.05230993 -0.35827946]\n",
      "[ 0.0149069   0.40544837 -0.05947552 -0.66698737]\n",
      "[ 0.02301587  0.60134483 -0.07281527 -0.977788  ]\n",
      "[ 0.03504277  0.40727077 -0.09237103 -0.70883709]\n",
      "[ 0.04318818  0.21354142 -0.10654777 -0.44660041]\n",
      "[ 0.04745901  0.40999662 -0.11547978 -0.77087791]\n",
      "[ 0.05565894  0.60650234 -0.13089734 -1.09754893]\n",
      "[ 0.06778899  0.41332369 -0.15284832 -0.84863389]\n",
      "[ 0.07605546  0.61016288 -0.16982099 -1.185213  ]\n",
      "[ 0.08825872  0.80703069 -0.19352525 -1.52595456]\n",
      "Episode finished after 19 timesteps\n",
      "[-0.02434426  0.0488197   0.00261191 -0.01612376]\n",
      "[-0.02336786 -0.14633962  0.00228944  0.27738211]\n",
      "[-0.02629466  0.0487496   0.00783708 -0.01457785]\n",
      "[-0.02531966  0.24375828  0.00754552 -0.30477781]\n",
      "[-0.0204445   0.4387719   0.00144997 -0.59507154]\n",
      "[-0.01166906  0.63387352 -0.01045146 -0.88729739]\n",
      "[ 1.00841043e-03  8.29135766e-01 -2.81974119e-02 -1.18324743e+00]\n",
      "[ 0.01759113  0.63439081 -0.05186236 -0.89953506]\n",
      "[ 0.03027894  0.44000861 -0.06985306 -0.62359448]\n",
      "[ 0.03907911  0.63603274 -0.08232495 -0.93743372]\n",
      "[ 0.05179977  0.83216248 -0.10107363 -1.25480816]\n",
      "[ 0.06844302  1.02842292 -0.12616979 -1.57736144]\n",
      "[ 0.08901148  1.22480221 -0.15771702 -1.90658447]\n",
      "[ 0.11350752  1.03169638 -0.19584871 -1.66669766]\n",
      "Episode finished after 14 timesteps\n",
      "[0.04933607 0.02217728 0.03414221 0.04307371]\n",
      "[ 0.04977962  0.21679343  0.03500368 -0.23864453]\n",
      "[0.05411549 0.02118936 0.03023079 0.06487083]\n",
      "[ 0.05453927 -0.17435269  0.03152821  0.36693638]\n",
      "[ 0.05105222 -0.36990814  0.03886694  0.66939154]\n",
      "[ 0.04365406 -0.17534754  0.05225477  0.38919505]\n",
      "[0.04014711 0.01899529 0.06003867 0.11343463]\n",
      "[ 0.04052701  0.21320785  0.06230736 -0.15971856]\n",
      "[ 0.04479117  0.407385    0.05911299 -0.43211257]\n",
      "[ 0.05293887  0.60162231  0.05047074 -0.70559054]\n",
      "[ 0.06497132  0.79600997  0.03635893 -0.98196864]\n",
      "[ 0.08089151  0.99062632  0.01671956 -1.26301306]\n",
      "[ 0.10070404  1.18553057 -0.00854071 -1.55041325]\n",
      "[ 0.12441465  0.99051211 -0.03954897 -1.26040716]\n",
      "[ 0.14422489  1.186117   -0.06475711 -1.56520944]\n",
      "[ 0.16794723  1.3819504  -0.0960613  -1.87736912]\n",
      "[ 0.19558624  1.57797979 -0.13360869 -2.19825617]\n",
      "[ 0.22714584  1.38437462 -0.17757381 -1.94960083]\n",
      "Episode finished after 18 timesteps\n",
      "[ 0.02269826 -0.03684509 -0.02777255 -0.01453423]\n",
      "[ 0.02196136  0.15866392 -0.02806323 -0.31584873]\n",
      "[ 0.02513463  0.35417413 -0.03438021 -0.61724809]\n",
      "[ 0.03221812  0.15954891 -0.04672517 -0.33558855]\n",
      "[ 0.0354091   0.35530361 -0.05343694 -0.64263233]\n",
      "[ 0.04251517  0.55112807 -0.06628959 -0.95165273]\n",
      "[ 0.05353773  0.35695784 -0.08532264 -0.68051195]\n",
      "[ 0.06067689  0.55315475 -0.09893288 -0.99879117]\n",
      "[ 0.07173998  0.35948452 -0.1189087  -0.7387452 ]\n",
      "[ 0.07892967  0.55603022 -0.13368361 -1.06635878]\n",
      "[ 0.09005028  0.75264319 -0.15501078 -1.39783288]\n",
      "[ 0.10510314  0.94931545 -0.18296744 -1.73469495]\n",
      "Episode finished after 12 timesteps\n",
      "[-0.04251872  0.03235092 -0.0208542  -0.02197011]\n",
      "[-0.0418717  -0.16246585 -0.0212936   0.26406081]\n",
      "[-0.04512102 -0.3572775  -0.01601239  0.54995219]\n",
      "[-0.05226657 -0.55217092 -0.00501334  0.8375474 ]\n",
      "[-0.06330999 -0.74722404  0.0117376   1.1286495 ]\n",
      "[-0.07825447 -0.55225779  0.03431059  0.83967104]\n",
      "[-0.08929962 -0.74783098  0.05110401  1.14294351]\n",
      "[-0.10425624 -0.55341268  0.07396288  0.86671482]\n",
      "[-0.1153245  -0.35937097  0.09129718  0.5981733 ]\n",
      "[-0.12251192 -0.5556438   0.10326065  0.91816051]\n",
      "[-0.13362479 -0.36205797  0.12162386  0.65963323]\n",
      "[-0.14086595 -0.16881976  0.13481652  0.40758643]\n",
      "[-0.14424235  0.02415876  0.14296825  0.16026137]\n",
      "[-0.14375917  0.21697531  0.14617348 -0.084123  ]\n",
      "[-0.13941967  0.40973244  0.14449102 -0.32735313]\n",
      "[-0.13122502  0.60253347  0.13794396 -0.5712083 ]\n",
      "[-0.11917435  0.4057742   0.12651979 -0.23844594]\n",
      "[-0.11105886  0.59888299  0.12175087 -0.48869621]\n",
      "[-0.0990812   0.79209583  0.11197695 -0.74066369]\n",
      "[-0.08323929  0.98550835  0.09716367 -0.99611303]\n",
      "[-0.06352912  0.78923075  0.07724141 -0.67456411]\n",
      "[-0.0477445   0.59312515  0.06375013 -0.35859637]\n",
      "[-0.035882    0.39715759  0.0565782  -0.0465123 ]\n",
      "[-0.02793885  0.20127191  0.05564796  0.26347118]\n",
      "[-0.02391341  0.39555722  0.06091738 -0.01115362]\n",
      "[-0.01600227  0.58975503  0.06069431 -0.28401138]\n",
      "[-0.00420717  0.78396116  0.05501408 -0.556951  ]\n",
      "[ 0.01147206  0.58811179  0.04387506 -0.24745541]\n",
      "[ 0.02323429  0.78258056  0.03892595 -0.52598272]\n",
      "[ 0.0388859  0.5869331  0.0284063 -0.2212925]\n",
      "[0.05062457 0.39141688 0.02398045 0.08021385]\n",
      "[0.0584529  0.19595952 0.02558472 0.38036517]\n",
      "[6.23720929e-02 4.83774704e-04 3.31920280e-02 6.81003910e-01]\n",
      "[0.06238177 0.19512938 0.04681211 0.39895298]\n",
      "[ 6.62843561e-02 -6.24310329e-04  5.47911658e-02  7.06019584e-01]\n",
      "[0.06627187 0.19369742 0.06891156 0.43107495]\n",
      "[0.07014582 0.38777935 0.07753306 0.16088699]\n",
      "[0.07790141 0.19163797 0.0807508  0.47698708]\n",
      "[0.08173416 0.38553246 0.09029054 0.21080802]\n",
      "[ 0.08944481  0.5792552   0.0945067  -0.05208306]\n",
      "[0.10102992 0.38291413 0.09346504 0.26885794]\n",
      "[0.1086882  0.18659125 0.0988422  0.58949566]\n",
      "[ 0.11242003 -0.00976566  0.11063211  0.91160574]\n",
      "[ 0.11222471 -0.20619661  0.12886422  1.23691037]\n",
      "[ 0.10810078 -0.01294407  0.15360243  0.98721641]\n",
      "[ 0.1078419  -0.20975185  0.17334676  1.32393409]\n",
      "[ 0.10364686 -0.40658693  0.19982544  1.66547086]\n",
      "Episode finished after 47 timesteps\n",
      "[ 0.04476606 -0.04440658 -0.00298654  0.0021544 ]\n",
      "[ 0.04387793  0.15075808 -0.00294345 -0.29146931]\n",
      "[ 0.04689309  0.34592187 -0.00877283 -0.58507911]\n",
      "[ 0.05381153  0.1509239  -0.02047442 -0.29517259]\n",
      "[ 0.05683001 -0.04390027 -0.02637787 -0.00901665]\n",
      "[ 0.055952    0.15158985 -0.0265582  -0.30990401]\n",
      "[ 0.0589838  -0.04314383 -0.03275628 -0.02571369]\n",
      "[ 0.05812092  0.15243218 -0.03327055 -0.32854896]\n",
      "[ 0.06116957  0.34801159 -0.03984153 -0.63153546]\n",
      "[ 0.0681298   0.15346751 -0.05247224 -0.35166122]\n",
      "[ 0.07119915 -0.0408705  -0.05950547 -0.07597518]\n",
      "[ 0.07038174  0.15505179 -0.06102497 -0.38682276]\n",
      "[ 0.07348277 -0.03915319 -0.06876143 -0.11398774]\n",
      "[ 0.07269971  0.15688321 -0.07104118 -0.42754738]\n",
      "[ 0.07583737 -0.03716445 -0.07959213 -0.15807862]\n",
      "[ 0.07509409  0.15900147 -0.0827537  -0.47477138]\n",
      "[ 0.07827411 -0.03486034 -0.09224913 -0.20927562]\n",
      "[ 0.07757691  0.16145128 -0.09643464 -0.52957378]\n",
      "[ 0.08080593 -0.0321912  -0.10702612 -0.26876748]\n",
      "[ 0.08016211 -0.22563595 -0.11240147 -0.01166629]\n",
      "[ 0.07564939 -0.41898147 -0.11263479  0.24354537]\n",
      "[ 0.06726976 -0.22244597 -0.10776389 -0.08243448]\n",
      "[ 0.06282084 -0.41587136 -0.10941257  0.17439924]\n",
      "[ 0.05450341 -0.21936744 -0.10592459 -0.15069727]\n",
      "[ 0.05011607 -0.41282573 -0.10893854  0.1067801 ]\n",
      "[ 0.04185955 -0.21632494 -0.10680293 -0.21818844]\n",
      "[ 0.03753305 -0.01985121 -0.1111667  -0.54256016]\n",
      "[ 0.03713603 -0.21324967 -0.12201791 -0.28686888]\n",
      "[ 0.03287103 -0.01661805 -0.12775528 -0.61540772]\n",
      "[ 0.03253867  0.18003558 -0.14006344 -0.94544214]\n",
      "[ 0.03613939  0.37673782 -0.15897228 -1.27865023]\n",
      "[ 0.04367414  0.57348817 -0.18454528 -1.6165956 ]\n",
      "Episode finished after 32 timesteps\n",
      "[-0.03457613  0.01590599 -0.0237286  -0.04331207]\n",
      "[-0.03425801 -0.17886781 -0.02459484  0.24179082]\n",
      "[-0.03783537  0.01659667 -0.01975903 -0.05854738]\n",
      "[-0.03750344 -0.17823648 -0.02092997  0.22783652]\n",
      "[-0.04106816 -0.37305317 -0.01637324  0.51384459]\n",
      "[-0.04852923 -0.17770449 -0.00609635  0.21604733]\n",
      "[-0.05208332 -0.37273876 -0.00177541  0.50680098]\n",
      "[-0.05953809 -0.17759184  0.00836061  0.21355909]\n",
      "[-0.06308993 -0.37283232  0.0126318   0.50886756]\n",
      "[-0.07054658 -0.17789059  0.02280915  0.22019195]\n",
      "[-0.07410439  0.01689802  0.02721299 -0.06520966]\n",
      "[-0.07376643 -0.1786033   0.02590879  0.23593339]\n",
      "[-0.07733849 -0.37408565  0.03062746  0.53667483]\n",
      "[-0.08482021 -0.56962455  0.04136096  0.83884869]\n",
      "[-0.0962127  -0.76528614  0.05813793  1.14424672]\n",
      "[-0.11151842 -0.57096987  0.08102286  0.8703477 ]\n",
      "[-0.12293782 -0.37703797  0.09842982  0.60419882]\n",
      "[-0.13047858 -0.5733887   0.1105138   0.92619098]\n",
      "[-0.14194635 -0.76981558  0.12903761  1.25145883]\n",
      "[-0.15734266 -0.96633293  0.15406679  1.58161601]\n",
      "[-0.17666932 -1.16291638  0.18569911  1.91811508]\n",
      "Episode finished after 21 timesteps\n",
      "[-0.04633471 -0.02877284 -0.01168115 -0.01528531]\n",
      "[-0.04691016 -0.22372533 -0.01198686  0.27368931]\n",
      "[-0.05138467 -0.02843442 -0.00651307 -0.02275007]\n",
      "[-0.05195336 -0.22346236 -0.00696808  0.26787081]\n",
      "[-0.05642261 -0.02824166 -0.00161066 -0.02700173]\n",
      "[-0.05698744  0.16690335 -0.00215069 -0.3201924 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05364937 -0.02818791 -0.00855454 -0.02818849]\n",
      "[-0.05421313  0.16705567 -0.00911831 -0.32355815]\n",
      "[-0.05087202 -0.02793526 -0.01558947 -0.03376465]\n",
      "[-0.05143072  0.16740674 -0.01626477 -0.33132517]\n",
      "[-0.04808259 -0.02747997 -0.02289127 -0.04381541]\n",
      "[-0.04863219 -0.22226631 -0.02376758  0.24155811]\n",
      "[-0.05307751 -0.41704083 -0.01893642  0.52665037]\n",
      "[-0.06141833 -0.22165762 -0.00840341  0.22806112]\n",
      "[-0.06585148 -0.02641659 -0.00384219 -0.06726066]\n",
      "[-0.06637981  0.16876023 -0.0051874  -0.36115333]\n",
      "[-0.06300461 -0.0262876  -0.01241047 -0.07011061]\n",
      "[-0.06353036 -0.22122945 -0.01381268  0.21863106]\n",
      "[-0.06795495 -0.0259128  -0.00944006 -0.07837676]\n",
      "[-0.06847321  0.1693432  -0.01100759 -0.37402304]\n",
      "[-0.06508634 -0.02562068 -0.01848805 -0.08483112]\n",
      "[-0.06559876  0.16976134 -0.02018468 -0.38328924]\n",
      "[-0.06220353 -0.02506829 -0.02785046 -0.09703821]\n",
      "[-0.06270489 -0.21978024 -0.02979123  0.18672947]\n",
      "[-0.0671005  -0.024245   -0.02605664 -0.11520057]\n",
      "[-0.0675854  -0.2189841  -0.02836065  0.16914913]\n",
      "[-0.07196508 -0.02346793 -0.02497766 -0.1323441 ]\n",
      "[-0.07243444  0.17200275 -0.02762455 -0.43280132]\n",
      "[-0.06899439  0.36750471 -0.03628057 -0.73406296]\n",
      "[-0.06164429  0.17290228 -0.05096183 -0.45301556]\n",
      "[-0.05818625 -0.02146336 -0.06002214 -0.1768219 ]\n",
      "[-0.05861551  0.17446392 -0.06355858 -0.4878195 ]\n",
      "[-0.05512623 -0.01970639 -0.07331497 -0.21582577]\n",
      "[-0.05552036 -0.21370783 -0.07763149  0.05285936]\n",
      "[-0.05979452 -0.40763577 -0.0765743   0.32007441]\n",
      "[-0.06794723 -0.21151154 -0.07017281  0.0042593 ]\n",
      "[-0.07217746 -0.40556061 -0.07008763  0.27400267]\n",
      "[-0.08028868 -0.59961619 -0.06460757  0.54378248]\n",
      "[-0.092281   -0.79377355 -0.05373192  0.8154295 ]\n",
      "[-0.10815647 -0.59795866 -0.03742333  0.50634148]\n",
      "[-0.12011564 -0.79253383 -0.0272965   0.78699999]\n",
      "[-0.13596632 -0.59704775 -0.0115565   0.48585593]\n",
      "[-0.14790728 -0.79200474 -0.00183938  0.77487435]\n",
      "[-0.16374737 -0.59685753  0.0136581   0.48161325]\n",
      "[-0.17568452 -0.79216958  0.02329037  0.77856936]\n",
      "[-0.19152791 -0.9876039   0.03886175  1.07848808]\n",
      "[-0.21127999 -1.18321693  0.06043152  1.38310857]\n",
      "[-0.23494433 -0.98889869  0.08809369  1.10991934]\n",
      "[-0.2547223  -1.18506067  0.11029207  1.42888736]\n",
      "[-0.27842352 -1.38135831  0.13886982  1.75390494]\n",
      "[-0.30605068 -1.57775557  0.17394792  2.0863614 ]\n",
      "Episode finished after 51 timesteps\n",
      "[ 0.0462727   0.03135821 -0.02927358 -0.00566329]\n",
      "[ 0.04689987 -0.16333195 -0.02938684  0.27764163]\n",
      "[ 0.04363323 -0.35802261 -0.02383401  0.56091303]\n",
      "[ 0.03647277 -0.55280209 -0.01261575  0.84599275]\n",
      "[ 0.02541673 -0.74774968  0.0043041   1.13468192]\n",
      "[ 0.01046174 -0.94292768  0.02699774  1.42871162]\n",
      "[-0.00839681 -0.74814936  0.05557198  1.14458697]\n",
      "[-0.0233598  -0.55379563  0.07846371  0.86983618]\n",
      "[-0.03443571 -0.35982369  0.09586044  0.60281813]\n",
      "[-0.04163219 -0.55614648  0.1079168   0.92409014]\n",
      "[-0.05275512 -0.36263476  0.1263986   0.66717714]\n",
      "[-0.06000781 -0.16947616  0.13974215  0.41681299]\n",
      "[-0.06339734 -0.36627325  0.14807841  0.75008165]\n",
      "[-0.0707228  -0.17347016  0.16308004  0.50741807]\n",
      "[-0.0741922   0.01902374  0.1732284   0.27024156]\n",
      "[-0.07381173  0.21130504  0.17863323  0.03681112]\n",
      "[-0.06958563  0.01413111  0.17936945  0.38010501]\n",
      "[-0.06930301 -0.18302412  0.18697155  0.72354552]\n",
      "[-0.07296349 -0.38017207  0.20144247  1.06876345]\n",
      "Episode finished after 19 timesteps\n",
      "[ 0.00721175  0.0315559  -0.01553044  0.00803706]\n",
      "[ 0.00784287 -0.16333992 -0.0153697   0.29577977]\n",
      "[ 0.00457607  0.03199773 -0.0094541  -0.00171061]\n",
      "[ 0.00521603  0.22725398 -0.00948831 -0.29736136]\n",
      "[ 0.00976111  0.4225099  -0.01543554 -0.59302157]\n",
      "[ 0.01821131  0.22760738 -0.02729597 -0.30524049]\n",
      "[ 0.02276345  0.42310747 -0.03340078 -0.6064054 ]\n",
      "[ 0.0312256   0.61868014 -0.04552889 -0.90941879]\n",
      "[ 0.04359921  0.42420299 -0.06371726 -0.63138623]\n",
      "[ 0.05208327  0.23002521 -0.07634499 -0.35943081]\n",
      "[ 0.05668377  0.03606685 -0.08353361 -0.09176437]\n",
      "[ 0.05740511  0.2322806  -0.08536889 -0.40958905]\n",
      "[ 0.06205072  0.03846617 -0.09356067 -0.14499411]\n",
      "[ 0.06282004 -0.15520004 -0.09646056  0.11676794]\n",
      "[ 0.05971604 -0.34881704 -0.0941252   0.37752789]\n",
      "[ 0.0527397  -0.54248495 -0.08657464  0.63911188]\n",
      "[ 0.04189    -0.34626938 -0.0737924   0.32046969]\n",
      "[ 0.03496461 -0.15017835 -0.06738301  0.00545833]\n",
      "[ 0.03196105 -0.34427247 -0.06727384  0.27614319]\n",
      "[ 0.0250756  -0.14825844 -0.06175098 -0.03697637]\n",
      "[ 0.02211043 -0.34244301 -0.0624905   0.23560206]\n",
      "[ 0.01526157 -0.14648649 -0.05777846 -0.07611925]\n",
      "[ 0.01233184  0.04941412 -0.05930085 -0.38645723]\n",
      "[ 0.01332012 -0.14481807 -0.06702999 -0.11304529]\n",
      "[ 0.01042376  0.05119713 -0.0692909  -0.42609988]\n",
      "[ 0.0114477  -0.14287846 -0.0778129  -0.15604029]\n",
      "[ 0.00859013  0.05326637 -0.0809337  -0.47222118]\n",
      "[ 0.00965546  0.24943257 -0.09037813 -0.78927584]\n",
      "[ 0.01464411  0.05566037 -0.10616364 -0.52633882]\n",
      "[ 0.01575732  0.25210341 -0.11669042 -0.85049791]\n",
      "[ 0.02079939  0.05874943 -0.13370038 -0.59666851]\n",
      "[ 0.02197438 -0.13427316 -0.14563375 -0.3489095 ]\n",
      "[ 0.01928891 -0.32705608 -0.15261194 -0.10546141]\n",
      "[ 0.01274779 -0.51969908 -0.15472117  0.13545061]\n",
      "[ 0.00235381 -0.32273845 -0.15201215 -0.2017679 ]\n",
      "[-0.00410096 -0.12580628 -0.15604751 -0.53827939]\n",
      "[-0.00661709  0.07112548 -0.1668131  -0.87577974]\n",
      "[-0.00519458 -0.12138416 -0.18432869 -0.63983955]\n",
      "[-0.00762226 -0.31352295 -0.19712548 -0.41040002]\n",
      "[-0.01389272 -0.11623251 -0.20533349 -0.75818476]\n",
      "Episode finished after 40 timesteps\n",
      "[-0.00303666 -0.04599847 -0.01858477  0.04253586]\n",
      "[-0.00395663 -0.24084906 -0.01773405  0.32929766]\n",
      "[-0.00877362 -0.0454792  -0.0111481   0.03107542]\n",
      "[-0.0096832  -0.24043952 -0.01052659  0.32022026]\n",
      "[-0.01449199 -0.43540999 -0.00412218  0.609565  ]\n",
      "[-0.02320019 -0.63047408  0.00806912  0.90094672]\n",
      "[-0.03580967 -0.43546238  0.02608805  0.610811  ]\n",
      "[-0.04451892 -0.63093908  0.03830427  0.91159528]\n",
      "[-0.0571377  -0.43635576  0.05653618  0.63119311]\n",
      "[-0.06586482 -0.63221908  0.06916004  0.94113145]\n",
      "[-0.0785092  -0.43809394  0.08798267  0.6709567 ]\n",
      "[-0.08727108 -0.63432174  0.1014018   0.98999344]\n",
      "[-0.09995751 -0.44069248  0.12120167  0.73080255]\n",
      "[-0.10877136 -0.24743547  0.13581772  0.47858975]\n",
      "[-0.11372007 -0.44418719  0.14538952  0.81080724]\n",
      "[-0.12260381 -0.64096984  0.16160566  1.14546108]\n",
      "[-0.13542321 -0.44828446  0.18451488  0.90750451]\n",
      "[-0.1443889  -0.64536016  0.20266497  1.25204021]\n",
      "Episode finished after 18 timesteps\n",
      "[ 0.04823213 -0.00848103 -0.01561172 -0.02064752]\n",
      "[ 0.04806251 -0.20337565 -0.01602467  0.26706913]\n",
      "[ 0.043995   -0.00802871 -0.01068329 -0.03062472]\n",
      "[ 0.04383442  0.18724481 -0.01129578 -0.3266591 ]\n",
      "[ 0.04757932  0.38252574 -0.01782896 -0.6228827 ]\n",
      "[ 0.05522984  0.57789204 -0.03028662 -0.92112702]\n",
      "[ 0.06678768  0.38319218 -0.04870916 -0.63811406]\n",
      "[ 0.07445152  0.57895828 -0.06147144 -0.94572976]\n",
      "[ 0.08603069  0.38471574 -0.08038604 -0.67297749]\n",
      "[ 0.093725    0.58085756 -0.09384559 -0.9898482 ]\n",
      "[ 0.10534215  0.38710852 -0.11364255 -0.72805357]\n",
      "[ 0.11308432  0.58360275 -0.12820362 -1.05423277]\n",
      "[ 0.12475638  0.3903918  -0.14928828 -0.80438353]\n",
      "[ 0.13256421  0.58721062 -0.16537595 -1.14005589]\n",
      "[ 0.14430843  0.39459061 -0.18817706 -0.90346709]\n",
      "[ 0.15220024  0.59169437 -0.20624641 -1.24890161]\n",
      "Episode finished after 16 timesteps\n",
      "[-0.03012415  0.02631331 -0.02954556  0.04681778]\n",
      "[-0.02959788  0.22184619 -0.02860921 -0.25503867]\n",
      "[-0.02516096  0.41736469 -0.03370998 -0.55660637]\n",
      "[-0.01681366  0.22273182 -0.04484211 -0.27473178]\n",
      "[-0.01235903  0.41846393 -0.05033674 -0.58121397]\n",
      "[-0.00398975  0.61425371 -0.06196102 -0.88931941]\n",
      "[ 0.00829532  0.81015926 -0.07974741 -1.20081871]\n",
      "[ 0.02449851  0.61615418 -0.10376378 -0.93415685]\n",
      "[ 0.03682159  0.81251127 -0.12244692 -1.25756013]\n",
      "[ 0.05307182  1.00896889 -0.14759812 -1.58594966]\n",
      "[ 0.0732512   0.81587779 -0.17931712 -1.34269866]\n",
      "[ 0.08956875  0.62340704 -0.20617109 -1.11105888]\n",
      "Episode finished after 12 timesteps\n",
      "[-0.00317652  0.04178054 -0.00729584 -0.04510345]\n",
      "[-0.00234091  0.23700635 -0.00819791 -0.34007932]\n",
      "[ 0.00239922  0.042002   -0.0149995  -0.04999278]\n",
      "[ 0.00323926  0.23733578 -0.01599936 -0.34737018]\n",
      "[ 0.00798598  0.04244501 -0.02294676 -0.05977505]\n",
      "[ 0.00883488  0.23788833 -0.02414226 -0.3596086 ]\n",
      "[ 0.01359264  0.433345   -0.03133443 -0.65980531]\n",
      "[ 0.02225954  0.23867281 -0.04453054 -0.37715118]\n",
      "[ 0.027033    0.43439799 -0.05207356 -0.68353557]\n",
      "[ 0.03572096  0.24003631 -0.06574427 -0.40769094]\n",
      "[ 0.04052169  0.04590517 -0.07389809 -0.13643822]\n",
      "[ 0.04143979  0.24200351 -0.07662686 -0.45148922]\n",
      "[ 0.04627986  0.43812079 -0.08565664 -0.76730816]\n",
      "[ 0.05504228  0.63431098 -0.1010028  -1.08566677]\n",
      "[ 0.0677285   0.44065584 -0.12271614 -0.82630885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07654161  0.63722305 -0.13924232 -1.1549304 ]\n",
      "[ 0.08928607  0.83385845 -0.16234092 -1.48783387]\n",
      "[ 0.10596324  0.64104352 -0.1920976  -1.24993314]\n",
      "Episode finished after 18 timesteps\n",
      "[ 0.03998683 -0.02898917 -0.0340523   0.0179484 ]\n",
      "[ 0.03940705 -0.22360665 -0.03369333  0.29969594]\n",
      "[ 0.03493492 -0.02802104 -0.02769941 -0.00341993]\n",
      "[ 0.0343745   0.16748698 -0.02776781 -0.30471205]\n",
      "[ 0.03772424  0.36299342 -0.03386205 -0.60602134]\n",
      "[ 0.0449841   0.16836092 -0.04598248 -0.3241935 ]\n",
      "[ 0.04835132  0.36410642 -0.05246635 -0.63101548]\n",
      "[ 0.05563345  0.55991966 -0.06508666 -0.9397492 ]\n",
      "[ 0.06683184  0.75585573 -0.08388164 -1.25215296]\n",
      "[ 0.08194896  0.95194616 -0.1089247  -1.56988671]\n",
      "[ 0.10098788  0.7582801  -0.14032244 -1.31306983]\n",
      "[ 0.11615348  0.56518538 -0.16658383 -1.0673942 ]\n",
      "[ 0.12745719  0.37261164 -0.18793172 -0.83128174]\n",
      "[ 0.13490942  0.18048662 -0.20455735 -0.60309567]\n",
      "Episode finished after 14 timesteps\n",
      "[ 0.01984832 -0.02518923  0.03476664  0.00900532]\n",
      "[ 0.01934453 -0.22079209  0.03494674  0.31245163]\n",
      "[ 0.01492869 -0.41639402  0.04119578  0.61594764]\n",
      "[ 0.00660081 -0.61206658  0.05351473  0.92131559]\n",
      "[-0.00564052 -0.41770704  0.07194104  0.64591946]\n",
      "[-0.01399466 -0.6137538   0.08485943  0.96036138]\n",
      "[-0.02626974 -0.41986867  0.10406666  0.69549894]\n",
      "[-0.03466711 -0.22633223  0.11797664  0.43730524]\n",
      "[-0.03919376 -0.03306054  0.12672274  0.18401763]\n",
      "[-0.03985497 -0.22974643  0.13040309  0.51383761]\n",
      "[-0.0444499  -0.0366789   0.14067985  0.2649251 ]\n",
      "[-0.04518347  0.15618416  0.14597835  0.01971206]\n",
      "[-0.04205979  0.34894381  0.14637259 -0.22358678]\n",
      "[-0.03508091  0.5417035   0.14190085 -0.4667544 ]\n",
      "[-0.02424684  0.7345654   0.13256577 -0.71156057]\n",
      "[-0.00955554  0.92762685  0.11833456 -0.95975158]\n",
      "[ 0.008997    1.12097642  0.09913952 -1.21303847]\n",
      "[ 0.03141653  1.31468909  0.07487875 -1.4730814 ]\n",
      "[ 0.05771031  1.1187359   0.04541713 -1.15798144]\n",
      "[ 0.08008503  0.9230524   0.0222575  -0.85141083]\n",
      "[ 0.09854608  1.11786393  0.00522928 -1.13701258]\n",
      "[ 0.12090336  1.31291709 -0.01751097 -1.42805093]\n",
      "[ 0.1471617   1.50825091 -0.04607199 -1.72615469]\n",
      "[ 0.17732672  1.70386842 -0.08059508 -2.03280997]\n",
      "[ 0.21140408  1.89972377 -0.12125128 -2.34930868]\n",
      "[ 0.24939856  1.70587996 -0.16823746 -2.09623833]\n",
      "Episode finished after 26 timesteps\n",
      "[-0.03560175 -0.02385725  0.03076115 -0.03101646]\n",
      "[-0.0360789  -0.21940652  0.03014082  0.27121103]\n",
      "[-0.04046703 -0.41494532  0.03556504  0.57324606]\n",
      "[-0.04876594 -0.22033959  0.04702996  0.29197599]\n",
      "[-0.05317273 -0.02591866  0.05286948  0.01448875]\n",
      "[-0.0536911   0.16840678  0.05315926 -0.26105573]\n",
      "[-0.05032296  0.36273118  0.04793814 -0.5365091 ]\n",
      "[-0.04306834  0.16696912  0.03720796 -0.22911495]\n",
      "[-0.03972896 -0.02866424  0.03262566  0.0750686 ]\n",
      "[-0.04030224 -0.22423835  0.03412703  0.3778639 ]\n",
      "[-0.04478701 -0.41982794  0.04168431  0.68110885]\n",
      "[-0.05318357 -0.61550329  0.05530649  0.98661855]\n",
      "[-0.06549364 -0.81132052  0.07503886  1.29614711]\n",
      "[-0.08172005 -1.00731113  0.1009618   1.61134644]\n",
      "[-0.10186627 -0.81351604  0.13318873  1.35176647]\n",
      "[-0.11813659 -1.01003529  0.16022406  1.68297645]\n",
      "[-0.13833729 -1.206609    0.19388359  2.02096491]\n",
      "Episode finished after 17 timesteps\n",
      "[-0.01161414 -0.02109425  0.01053917 -0.03868592]\n",
      "[-0.01203603 -0.21636573  0.00976545  0.2573035 ]\n",
      "[-0.01636334 -0.02138455  0.01491152 -0.03228333]\n",
      "[-0.01679103  0.17352042  0.01426586 -0.32022446]\n",
      "[-0.01332063  0.36843633  0.00786137 -0.60837456]\n",
      "[-0.0059519   0.5634475  -0.00430612 -0.89857105]\n",
      "[ 0.00531705  0.36838418 -0.02227754 -0.60724478]\n",
      "[ 0.01268474  0.17358067 -0.03442244 -0.32166108]\n",
      "[ 0.01615635 -0.02103461 -0.04085566 -0.04002929]\n",
      "[ 0.01573566 -0.2155476  -0.04165625  0.2394885 ]\n",
      "[ 0.0114247  -0.41005048 -0.03686648  0.51874633]\n",
      "[ 0.00322369 -0.60463453 -0.02649155  0.79958778]\n",
      "[-0.008869   -0.79938325 -0.0104998   1.08382076]\n",
      "[-0.02485666 -0.99436509  0.01117662  1.37319051]\n",
      "[-0.04474396 -0.79938464  0.03864043  1.08402396]\n",
      "[-0.06073166 -0.60479326  0.06032091  0.80371222]\n",
      "[-0.07282752 -0.41054796  0.07639515  0.5305978 ]\n",
      "[-0.08103848 -0.21657908  0.08700711  0.26293143]\n",
      "[-0.08537006 -0.41282826  0.09226574  0.58173949]\n",
      "[-0.09362663 -0.21911194  0.10390053  0.31948862]\n",
      "[-0.09800887 -0.41554834  0.1102903   0.64304603]\n",
      "[-0.10631983 -0.61202065  0.12315122  0.96832356]\n",
      "[-0.11856025 -0.41874809  0.14251769  0.71672536]\n",
      "[-0.12693521 -0.61552474  0.1568522   1.05065631]\n",
      "[-0.1392457  -0.42279145  0.17786533  0.81103109]\n",
      "[-0.14770153 -0.23049381  0.19408595  0.57915239]\n",
      "[-0.15231141 -0.4277299   0.20566899  0.92615538]\n",
      "Episode finished after 27 timesteps\n",
      "[ 0.03645669 -0.00443609  0.03831733 -0.03185483]\n",
      "[ 0.03636797  0.19011602  0.03768024 -0.31220625]\n",
      "[ 0.04017029 -0.00552191  0.03143611 -0.00788228]\n",
      "[ 0.04005985 -0.20108027  0.03127847  0.29455101]\n",
      "[ 0.03603825 -0.00641789  0.03716949  0.01189464]\n",
      "[ 0.03590989  0.18815183  0.03740738 -0.26883305]\n",
      "[ 0.03967292  0.38272053  0.03203072 -0.54948672]\n",
      "[ 0.04732734  0.57737826  0.02104098 -0.83190811]\n",
      "[ 0.0588749   0.38197517  0.00440282 -0.53268277]\n",
      "[ 0.0665144   0.18679157 -0.00625083 -0.23861579]\n",
      "[ 0.07025024  0.38200226 -0.01102315 -0.53326383]\n",
      "[ 0.07789028  0.5772775  -0.02168843 -0.82939963]\n",
      "[ 0.08943583  0.77268911 -0.03827642 -1.12882401]\n",
      "[ 0.10488961  0.96829094 -0.0608529  -1.43326237]\n",
      "[ 0.12425543  0.77397026 -0.08951815 -1.16020058]\n",
      "[ 0.13973484  0.58012124 -0.11272216 -0.89687453]\n",
      "[ 0.15133726  0.38669303 -0.13065965 -0.64164376]\n",
      "[ 0.15907112  0.19361117 -0.14349252 -0.39279439]\n",
      "[ 0.16294335  0.39044683 -0.15134841 -0.72705563]\n",
      "[ 0.17075228  0.58730116 -0.16588952 -1.06328842]\n",
      "[ 0.18249831  0.78418384 -0.18715529 -1.40310632]\n",
      "Episode finished after 21 timesteps\n",
      "[-0.04005874 -0.00720698  0.01540234 -0.00107249]\n",
      "[-0.04020288 -0.2025464   0.01538089  0.29642999]\n",
      "[-0.04425381 -0.00764705  0.02130949  0.00863736]\n",
      "[-0.04440675 -0.20306803  0.02148224  0.30796678]\n",
      "[-0.04846811 -0.00825866  0.02764158  0.02213545]\n",
      "[-0.04863329 -0.20376589  0.02808428  0.32340985]\n",
      "[-0.0527086  -0.00905486  0.03455248  0.03971418]\n",
      "[-0.0528897   0.18555501  0.03534677 -0.24186998]\n",
      "[-0.0491786   0.38015469  0.03050937 -0.52319739]\n",
      "[-0.04157551  0.18461693  0.02004542 -0.22105887]\n",
      "[-0.03788317 -0.01078573  0.01562424  0.07787918]\n",
      "[-0.03809888  0.1841088   0.01718182 -0.20983358]\n",
      "[-0.03441671  0.37898091  0.01298515 -0.4970474 ]\n",
      "[-0.02683709  0.57391738  0.0030442  -0.78560993]\n",
      "[-0.01535874  0.76899738 -0.01266799 -1.07733357]\n",
      "[ 2.12071461e-05  9.64284349e-01 -3.42146653e-02 -1.37396481e+00]\n",
      "[ 0.01930689  1.15981686 -0.06169396 -1.67714907]\n",
      "[ 0.04250323  1.3555977  -0.09523694 -1.98838815]\n",
      "[ 0.06961519  1.16159491 -0.13500471 -1.72666004]\n",
      "[ 0.09284708  0.96825048 -0.16953791 -1.47885327]\n",
      "[ 0.11221209  1.1649877  -0.19911497 -1.81933383]\n",
      "Episode finished after 21 timesteps\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
