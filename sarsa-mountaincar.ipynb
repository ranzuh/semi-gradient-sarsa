{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tiles3 as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(q_values):\n",
    "    top = float(\"-inf\")\n",
    "    ties = []\n",
    "\n",
    "    for i in range(len(q_values)):\n",
    "        if q_values[i] > top:\n",
    "            top = q_values[i]\n",
    "            ties = []\n",
    "\n",
    "        if q_values[i] == top:\n",
    "            ties.append(i)\n",
    "\n",
    "    return np.random.choice(ties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCarTileCoder:\n",
    "    def __init__(self, iht_size=4096, num_tilings=8, num_tiles=8):\n",
    "        \"\"\"\n",
    "        Initializes the MountainCar Tile Coder\n",
    "        Initializers:\n",
    "        iht_size -- int, the size of the index hash table, typically a power of 2\n",
    "        num_tilings -- int, the number of tilings\n",
    "        num_tiles -- int, the number of tiles. Here both the width and height of the\n",
    "                     tile coder are the same\n",
    "        Class Variables:\n",
    "        self.iht -- tc.IHT, the index hash table that the tile coder will use\n",
    "        self.num_tilings -- int, the number of tilings the tile coder will use\n",
    "        self.num_tiles -- int, the number of tiles the tile coder will use\n",
    "        \"\"\"\n",
    "        self.iht = tc.IHT(iht_size)\n",
    "        self.num_tilings = num_tilings\n",
    "        self.num_tiles = num_tiles\n",
    "    \n",
    "    def get_tiles(self, position, velocity):\n",
    "        \"\"\"\n",
    "        Takes in a position and velocity from the mountaincar environment\n",
    "        and returns a numpy array of active tiles.\n",
    "        \n",
    "        Arguments:\n",
    "        position -- float, the position of the agent between -1.2 and 0.5\n",
    "        velocity -- float, the velocity of the agent between -0.07 and 0.07\n",
    "        returns:\n",
    "        tiles - np.array, active tiles\n",
    "        \"\"\"\n",
    "\n",
    "        POSITION_MIN = -1.2\n",
    "        POSITION_MAX = 0.6\n",
    "        VELOCITY_MIN = -0.07\n",
    "        VELOCITY_MAX = 0.07\n",
    "\n",
    "        position_scale = self.num_tiles / (POSITION_MAX - POSITION_MIN)\n",
    "        velocity_scale = self.num_tiles / (VELOCITY_MAX - VELOCITY_MIN)\n",
    "\n",
    "        tiles = tc.tiles(self.iht, self.num_tilings, [position * position_scale, \n",
    "                                                      velocity * velocity_scale])\n",
    "        \n",
    "        return np.array(tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "    \n",
    "    num_tilings = 8\n",
    "    num_tiles = 8\n",
    "    iht_size = 4096 # index hash table\n",
    "    \n",
    "    # learning rate\n",
    "    alpha = 0.5 / num_tilings\n",
    "    # how often random move\n",
    "    epsilon = 0\n",
    "    # discount future rewards\n",
    "    gamma = 1\n",
    "    \n",
    "    \n",
    "\n",
    "    def __init__(self, action_space, observation_space):\n",
    "\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        # initialize value-function weights w\n",
    "        self.w = np.ones((action_space.n, self.iht_size))\n",
    "        \n",
    "        # initialize tilecoder\n",
    "        self.tc = MountainCarTileCoder(iht_size=self.iht_size, \n",
    "                                       num_tilings=self.num_tilings, \n",
    "                                       num_tiles=self.num_tiles)\n",
    "        \n",
    "        self.previous_tiles = None\n",
    "        \n",
    "        # initialize state, action and reward\n",
    "        #self.s = self.a = self.r = None\n",
    "    \n",
    "    def select_initial_action(self, state):\n",
    "        position, velocity = state\n",
    "        \n",
    "        active_tiles = self.tc.get_tiles(position, velocity)\n",
    "        action, action_value = self.select_action(active_tiles)\n",
    "        \n",
    "        self.previous_tiles = np.copy(active_tiles)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def select_action(self, tiles):\n",
    "        action_values = []\n",
    "        action = None\n",
    "        # First loop through the weights of each action and populate action_values\n",
    "        # with the action value for each action and tiles instance\n",
    "        for w in self.w:\n",
    "            action_values.append(sum(w[tiles]))\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(len(action_values))\n",
    "        else:\n",
    "            action = argmax(action_values)\n",
    "        \n",
    "        #print(\"tiles\", tiles, \"action_values\", action_values)\n",
    "        #print(\"weights\", self.w)\n",
    "        \n",
    "        return action, action_values[action]\n",
    "\n",
    "    def observe(self, state, action, reward, next_state, done):\n",
    "            #input()\n",
    "            #print(\"state\", state, \"action\", action, \"reward\", reward, \"next_state\", next_state)\n",
    "            if done:\n",
    "                target = reward\n",
    "                estimate = sum(self.w[action][self.previous_tiles])\n",
    "                self.w[action][self.previous_tiles] += self.alpha * (target - estimate)\n",
    "                return None\n",
    "            else:\n",
    "                position, velocity = next_state\n",
    "                active_tiles = self.tc.get_tiles(position, velocity)\n",
    "                \n",
    "                next_action, action_value = self.select_action(active_tiles)\n",
    "                \n",
    "                target = reward + self.gamma * action_value\n",
    "                estimate = sum(self.w[action][self.previous_tiles])\n",
    "                # print(\"loss\", target - estimate)\n",
    "                \n",
    "                self.w[action][self.previous_tiles] += self.alpha * (target - estimate)\n",
    "                \n",
    "                self.previous_tiles = np.copy(active_tiles)\n",
    "            return next_action\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-111.76"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "#env = env.unwrapped\n",
    "agent = SarsaAgent(env.action_space, env.observation_space)\n",
    "\n",
    "total_rewards = []\n",
    "\n",
    "for i_episode in range(1000):\n",
    "    # choose inital state and action\n",
    "    state = env.reset()\n",
    "    action = agent.select_initial_action(state)\n",
    "    total_reward = 0\n",
    "    \n",
    "    for t in range(200):\n",
    "        # env.render()\n",
    "        # Take action and observe reward, next_state\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        action = agent.observe(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            #print(\"Episode finished after {} timesteps with reward {}\".format(t+1, total_reward))\n",
    "            break\n",
    "        state = next_state\n",
    "    \n",
    "    total_rewards.append(total_reward)\n",
    "    if i_episode >= 100 and np.mean(total_rewards[-100:]) > -105:\n",
    "        print(\"Solved, episode\", i_episode)\n",
    "        break\n",
    "    \n",
    "env.close()\n",
    "\n",
    "np.mean(total_rewards[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 142\n",
      "Episode finished after 100\n",
      "Episode finished after 92\n",
      "Episode finished after 162\n",
      "Episode finished after 105\n",
      "Episode finished after 92\n",
      "Episode finished after 106\n",
      "Episode finished after 155\n",
      "Episode finished after 142\n",
      "Episode finished after 89\n"
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "\n",
    "for i_episode in range(10):\n",
    "    # choose inital state and action\n",
    "    state = env.reset()\n",
    "    action = agent.select_initial_action(state)\n",
    "    total_reward = 0\n",
    "    \n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        # Take action and observe reward, next_state\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        action = agent.observe(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            print(\"Episode finished after {}\".format(t+1))\n",
    "            break\n",
    "        state = next_state\n",
    "    \n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "np.mean(total_rewards)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
