{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tiles3 as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(q_values):\n",
    "    top = float(\"-inf\")\n",
    "    ties = []\n",
    "\n",
    "    for i in range(len(q_values)):\n",
    "        if q_values[i] > top:\n",
    "            top = q_values[i]\n",
    "            ties = []\n",
    "\n",
    "        if q_values[i] == top:\n",
    "            ties.append(i)\n",
    "\n",
    "    return np.random.choice(ties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCarTileCoder:\n",
    "    def __init__(self, iht_size=4096, num_tilings=8, num_tiles=8):\n",
    "        \"\"\"\n",
    "        Initializes the MountainCar Tile Coder\n",
    "        Initializers:\n",
    "        iht_size -- int, the size of the index hash table, typically a power of 2\n",
    "        num_tilings -- int, the number of tilings\n",
    "        num_tiles -- int, the number of tiles. Here both the width and height of the\n",
    "                     tile coder are the same\n",
    "        Class Variables:\n",
    "        self.iht -- tc.IHT, the index hash table that the tile coder will use\n",
    "        self.num_tilings -- int, the number of tilings the tile coder will use\n",
    "        self.num_tiles -- int, the number of tiles the tile coder will use\n",
    "        \"\"\"\n",
    "        self.iht = tc.IHT(iht_size)\n",
    "        self.num_tilings = num_tilings\n",
    "        self.num_tiles = num_tiles\n",
    "    \n",
    "    def get_tiles(self, position, velocity):\n",
    "        \"\"\"\n",
    "        Takes in a position and velocity from the mountaincar environment\n",
    "        and returns a numpy array of active tiles.\n",
    "        \n",
    "        Arguments:\n",
    "        position -- float, the position of the agent between -1.2 and 0.5\n",
    "        velocity -- float, the velocity of the agent between -0.07 and 0.07\n",
    "        returns:\n",
    "        tiles - np.array, active tiles\n",
    "        \"\"\"\n",
    "\n",
    "        CART_POSITION_MIN = -1.2\n",
    "        CART_POSITION_MAX = 0.6\n",
    "        CART_VELOCITY_MIN = -0.07\n",
    "        CART_VELOCITY_MAX = 0.07\n",
    "\n",
    "        position_scale = self.num_tiles / (POSITION_MAX - POSITION_MIN)\n",
    "        velocity_scale = self.num_tiles / (VELOCITY_MAX - VELOCITY_MIN)\n",
    "\n",
    "        tiles = tc.tiles(self.iht, self.num_tilings, [position * position_scale, \n",
    "                                                      velocity * velocity_scale])\n",
    "        \n",
    "        return np.array(tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "    \n",
    "    num_tilings = 8\n",
    "    num_tiles = 8\n",
    "    iht_size = 4096 # index hash table\n",
    "    \n",
    "    # learning rate\n",
    "    alpha = 0.5 / num_tilings\n",
    "    # how often random move\n",
    "    epsilon = 0\n",
    "    # discount future rewards\n",
    "    gamma = 1\n",
    "    \n",
    "    \n",
    "\n",
    "    def __init__(self, action_space, observation_space):\n",
    "\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        # initialize value-function weights w\n",
    "        self.w = np.ones((action_space.n, self.iht_size))\n",
    "        \n",
    "        # initialize tilecoder\n",
    "        self.tc = MountainCarTileCoder(iht_size=self.iht_size, \n",
    "                                       num_tilings=self.num_tilings, \n",
    "                                       num_tiles=self.num_tiles)\n",
    "        \n",
    "        self.previous_tiles = None\n",
    "        \n",
    "        # initialize state, action and reward\n",
    "        #self.s = self.a = self.r = None\n",
    "    \n",
    "    def select_initial_action(self, state):\n",
    "        position, velocity = state\n",
    "        \n",
    "        active_tiles = self.tc.get_tiles(position, velocity)\n",
    "        action, action_value = self.select_action(active_tiles)\n",
    "        \n",
    "        self.previous_tiles = np.copy(active_tiles)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def select_action(self, tiles):\n",
    "        action_values = []\n",
    "        action = None\n",
    "        # First loop through the weights of each action and populate action_values\n",
    "        # with the action value for each action and tiles instance\n",
    "        for w in self.w:\n",
    "            action_values.append(sum(w[tiles]))\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(len(action_values))\n",
    "        else:\n",
    "            action = argmax(action_values)\n",
    "        \n",
    "        #print(\"tiles\", tiles, \"action_values\", action_values)\n",
    "        #print(\"weights\", self.w)\n",
    "        \n",
    "        return action, action_values[action]\n",
    "\n",
    "    def observe(self, state, action, reward, next_state, done):\n",
    "            #input()\n",
    "            #print(\"state\", state, \"action\", action, \"reward\", reward, \"next_state\", next_state)\n",
    "            if done:\n",
    "                target = reward\n",
    "                estimate = sum(self.w[action][self.previous_tiles])\n",
    "                self.w[action][self.previous_tiles] += self.alpha * (target - estimate)\n",
    "                return None\n",
    "            else:\n",
    "                position, velocity = next_state\n",
    "                active_tiles = self.tc.get_tiles(position, velocity)\n",
    "                \n",
    "                next_action, action_value = self.select_action(active_tiles)\n",
    "                \n",
    "                target = reward + self.gamma * action_value\n",
    "                estimate = sum(self.w[action][self.previous_tiles])\n",
    "                # print(\"loss\", target - estimate)\n",
    "                \n",
    "                self.w[action][self.previous_tiles] += self.alpha * (target - estimate)\n",
    "                \n",
    "                self.previous_tiles = np.copy(active_tiles)\n",
    "            return next_action\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-f02511b25713>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# Take action and observe reward, next_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-e5bde0bb1088>\u001b[0m in \u001b[0;36mobserve\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[0mactive_tiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tiles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvelocity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                 \u001b[0mnext_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactive_tiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0maction_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-e5bde0bb1088>\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(self, tiles)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# with the action value for each action and tiles instance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0maction_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtiles\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;31m# Epsilon-greedy action selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "#env = env.unwrapped\n",
    "agent = SarsaAgent(env.action_space, env.observation_space)\n",
    "\n",
    "total_rewards = []\n",
    "\n",
    "for i_episode in range(10000):\n",
    "    # choose inital state and action\n",
    "    state = env.reset()\n",
    "    action = agent.select_initial_action(state)\n",
    "    total_reward = 0\n",
    "    \n",
    "    for t in range(200):\n",
    "        # env.render()\n",
    "        # Take action and observe reward, next_state\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        action = agent.observe(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            #print(\"Episode finished after {} timesteps with reward {}\".format(t+1, total_reward))\n",
    "            break\n",
    "        state = next_state\n",
    "    \n",
    "    total_rewards.append(total_reward)\n",
    "    if i_episode >= 100 and np.mean(total_rewards[-100:]) > -105:\n",
    "        print(\"Solved, episode\", i_episode)\n",
    "        break\n",
    "    \n",
    "env.close()\n",
    "\n",
    "np.mean(total_rewards[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 106\n",
      "Episode finished after 112\n",
      "Episode finished after 106\n",
      "Episode finished after 105\n",
      "Episode finished after 141\n",
      "Episode finished after 106\n",
      "Episode finished after 144\n",
      "Episode finished after 165\n",
      "Episode finished after 146\n",
      "Episode finished after 140\n"
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "\n",
    "for i_episode in range(10):\n",
    "    # choose inital state and action\n",
    "    state = env.reset()\n",
    "    action = agent.select_initial_action(state)\n",
    "    total_reward = 0\n",
    "    \n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        # Take action and observe reward, next_state\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        action = agent.observe(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            print(\"Episode finished after {}\".format(t+1))\n",
    "            break\n",
    "        state = next_state\n",
    "    \n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "np.mean(total_rewards)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
